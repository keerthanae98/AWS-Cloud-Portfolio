# ğŸ“š Week 3 â€“ Academic Dataset Ingestion, Analysis & Lifecycle Management

## ğŸ” Overview

This project demonstrates the end-to-end pipeline for academic data ingestion, profiling, transformation, and lifecycle configuration using **AWS S3**, **AWS Glue DataBrew**, and **EC2**. The solution is built to simulate a scalable academic data analysis platform that ingests CSV datasets, transforms and profiles them using DataBrew, and manages cost-efficiency using intelligent storage and lifecycle rules.

---

## ğŸ“ Datasets Used

| Dataset Name                     | File                          | Description                                  |
|----------------------------------|-------------------------------|----------------------------------------------|
| Faculty Member List              | `facultymember-list.csv`      | Faculty member details including experience. |
| Scholarly Activity List          | `scholarlyactivity-list.csv`  | Research publications and scholarly records. |
| Activity Committee Members List | `activitycommitteemembers-list.csv` | Committee member records for university activities. |

---

## ğŸ§± Architecture Summary

- Raw datasets ingested into: `academics-raw-kee`
- Transformed & profiled outputs saved to: `academics-trf-kee`
- Processing done via: **AWS Glue DataBrew**
- Output format: **CSV** and **Parquet** (with Snappy compression)
- Lifecycle policies applied for storage class transitions

---

## ğŸ“Š Data Profiling & Cleansing

All datasets were profiled using AWS Glue DataBrew to:
- Check for missing values
- Determine data types
- Identify correlations (e.g., `Years_of_Experience`)
- Cleanse and normalize data
- Output to S3 in CSV & Parquet

**Data Quality Summary**  
| Dataset                  | Total Rows | Total Columns | Missing Cells | Cleansing Job Status |
|--------------------------|------------|----------------|----------------|------------------------|
| Faculty List             | 50         | 11             | 0              | âœ… Succeeded           |
| Scholarly Activity       | 100        | 11             | 5%             | âœ… Succeeded           |
| Activity Committee List  | 50         | 10             | 0              | âœ… Succeeded           |

---

## ğŸ”„ Lifecycle Rules Applied

Lifecycle policies were created on the `academics-raw-kee` bucket for each dataset to move infrequently accessed data to **Glacier/Glacier Instant Retrieval**, reducing storage cost over time.

| Lifecycle Rule Name              | Transition Target          |
|----------------------------------|-----------------------------|
| `academics-fm-lst-lr-kee`        | Glacier Instant Retrieval   |
| `academics-sha-lst-lr-kee`       | Glacier Flexible Retrieval  |
| `academics-acm-lst-lr-kee`       | Glacier Flexible Retrieval  |

---

## ğŸ” Logs and Access Monitoring

- Server logs captured from EC2 Windows instance (`IIS` logs)
- Sample logs stored under `responsibilities/academics-user-log/year=2025/...`
- EC2 instance used PowerShell and `Write-S3Object` (note: required credentials setup to avoid failure)

---

## ğŸ§  Learnings and Outcomes

- Practical use of **DataBrew** for no-code profiling and transformation  
- Efficient storage with **S3 Lifecycle Rules**  
- Real-world simulation of academic data team ingestion pipelines  
- Use of **CSV and Parquet** formats for flexibility and compression  
- Error handling during PowerShell S3 uploads  

---

## ğŸ“Œ Screenshots

All screenshots documenting each step are stored in this directory as `image1.png` to `image13.png`.

---

## âœ… Completion Status

| Task                                | Status      |
|-------------------------------------|-------------|
| S3 Bucket Creation                  | âœ… Done      |
| Data Upload (Raw)                   | âœ… Done      |
| Glue DataBrew Profiling             | âœ… Done      |
| Data Cleansing Job (CSV + Parquet) | âœ… Done      |
| Lifecycle Rules                     | âœ… Done      |
| Logs and EC2 CLI Execution          | âœ… Done      |
| Documentation and Diagram (draw.io) | âœ… Done      |

---

## ğŸ”— Output Paths

Example S3 output folder: s3://academics-trf-kee/responsibilities/facultymember-list/user/

csharp 
Parquet output with compression:

s3://academics-trf-kee/responsibilities/facultymember-list/system/

---

## ğŸ‘©â€ğŸ’» Created By

**Keerthana A.** â€“ AWS Academy Cloud Foundations  
Project: Cloud Computing Portfolio â€“ Week 3  


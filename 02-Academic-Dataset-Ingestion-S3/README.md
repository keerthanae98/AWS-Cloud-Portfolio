# ğŸ“ Week 2 â€“ Academic Dataset Ingestion using Amazon S3

## ğŸ“ Objective:
To upload and organize academic CSV datasets into Amazon S3 using structured folders and access via internet-connected academic teams.

---

## ğŸ§° AWS Services Used:
- **Amazon S3** â€“ For storing CSV files
- **AWS CLI (via PowerShell)** â€“ For uploading files with folder structures
- **S3 Bucket Folder Structure** â€“ To organize datasets by year, type, and server

---

## ğŸ§ª What I Did:
- Collected 3 academic CSV files:
  - `faculty-member-list.csv`
  - `scholarlyactivity-list.csv`
  - `activitycommitteemembers-list.csv`
- Created the folder path in S3: responsibilities/facultymember-list/year-2025/server=AGVS-Kee/
- Used PowerShell + `Write-S3Object` command to upload each file.
- Verified successful upload using the S3 web console.

---

## ğŸ“¸ Screenshots:
- ğŸ“ Folder structure in S3
- ğŸ“„ CSV schemas (ERD)
- âš™ï¸ CLI commands & errors
- âœ… Final upload confirmation in S3

---

## ğŸ§  Learnings:
- Got hands-on with CLI-based upload.
- Understood folder-based key structuring in S3.
- Practiced organizing semi-structured data in buckets.

---

## ğŸ”— Project Files:
- All screenshots and CSV files are available in this folder.
